{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a7408312c6fef42dc9b1c6805d9f11fd",
     "grade": false,
     "grade_id": "cell-035626ace7d8926c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Anomaly and Outlier Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: Most text is excerpted verbatim from references listed at the end. Some exercises are adapted from the course from Andrew NG on Coursera. This notebook is not intended for publication, it is a class reference material._\n",
    "\n",
    "Anomaly detection is the process of identifying unexpected items or events in datasets, which differ from the norm. In contrast to standard classification tasks, anomaly detection is often applied on unlabeled data, taking only the internal structure of the dataset into account. Anomalies are often associated with particular interesting events or suspicious data records. Anomalies are known to have two important characteristics:\n",
    "\n",
    "1. Anomalies are different from the norm with respect to their features and\n",
    "2. They are rare in a dataset compared to normal instances.\n",
    "\n",
    "Anomaly detection algorithms are now used in many application domains and often enhance traditional rule-based detection systems, for example:\n",
    "- Intrusion detection\n",
    "- Fraud detection\n",
    "- Data Leakage Prevention\n",
    "\n",
    "We can distinguish between three main types of anomaly detection: Supervised Anomaly Detection, Semi-supervised Anomaly Detection, and Unsupervised Anomaly Detection. Unsupervised Anomaly Detection is the most flexible setup which does not require any labels. Furthermore, there is also no distinction between a training and a test dataset. The idea is that an unsupervised anomaly detection algorithm scores the data solely based on intrinsic properties of the dataset. Typically, distances or densities are used to give an estimation what is normal and what is an outlier. \n",
    "\n",
    " <figure>\n",
    "  <img src=\"Figures/journal.pone.0152173.g001.PNG\" alt=\"Types\" style=\"width: 700px\">\n",
    "  <figcaption>Fig 1. Different anomaly detection modes depending on the availability of labels in the dataset.\n",
    "(a) Supervised anomaly detection uses a fully labeled dataset for training. (b) Semi-supervised anomaly detection uses an anomaly-free training dataset. Afterwards, deviations in the test data from that normal model are used to detect anomalies. (c) Unsupervised anomaly detection algorithms use only intrinsic information of the data in order to detect instances deviating from the majority of the data.\n",
    "https://doi.org/10.1371/journal.pone.0152173.g001\n",
    "    </figcaption>\n",
    "</figure> \n",
    "\n",
    "The output of an unsupervised anomaly detection algorithm is often a score. Here, we also use scores and rank the results such that the ranking can be used for performance evaluation. Of course, a ranking can be converted into a label using an appropriate threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of anomalies\n",
    "\n",
    " <figure>\n",
    "  <img src=\"Figures/journal.pone.0152173.g002.PNG\" alt=\"Types of anomaly\" style=\"width: 700px\">\n",
    "  <figcaption>Fig 2. A simple two-dimensional example.\n",
    "It illustrates global anomalies (x1, x2), a local anomaly x3 and a micro-cluster c3.\n",
    "https://doi.org/10.1371/journal.pone.0152173.g002\n",
    "    </figcaption>\n",
    "</figure> \n",
    "\n",
    "In Fig 2 two anomalies can be easily identified by eye: x1 and x2 are very different from the dense areas with respect to their attributes and are therefore called global anomalies. When looking at the dataset globally, x3 can be seen as a normal record since it is not too far away from the cluster c2. However, when we focus only on the cluster c2 and compare it with x3 while neglecting all the other instances, it can be seen as an anomaly. Therefore, x3 is called a local anomaly, since it is only anomalous when compared with its close-by neighborhood. It depends on the application, whether local anomalies are of interest or not. Another interesting question is whether the instances of the cluster c3 should be seen as three anomalies or as a (small) regular cluster. These phenomena is called micro cluster and anomaly detection algorithms should assign scores to its members larger than the normal instances, but smaller values than the obvious anomalies. This simple example already illustrates that anomalies are not always obvious and a score is much more useful than a binary label assignment.\n",
    "\n",
    "To this end, an anomaly is always referred to a single instance in a dataset only occurring rarely. In reality, this is often not true. For example, in intrusion detection, anomalies are often referred to many (suspicious) access patterns, which may be observed at a larger amount as the normal accesses. In this case, an unsupervised anomaly detection algorithm directly applied on the raw data will fail. The task of detecting single anomalous instances in a larger dataset (as introduced so far) is called point anomaly detection. Nearly all available unsupervised anomaly detection algorithms today are from this type. If an anomalous situation is represented as a set of many instances, this is called a collective anomaly. Each of these instances is not necessarily a point anomaly, but only a specific combination thereof defines the anomaly. The previous given example of occurring multiple specific access patterns in intrusion detection is such a collective anomaly. A third kind are contextual anomalies, which describe the effect that a point can be seen as normal, but when a given context is taken into account, the point turns out to be an anomaly. The most commonly occurring context is time. As an example, suppose we measure temperature in a range of 0°to 35°C during the year. Thus, a temperature of 26°C seems pretty normal, but when we take the context time into account (e.g. the month), such a high temperature of 26°C during winter would definitively be considered as an anomaly.\n",
    "\n",
    " <figure>\n",
    "  <img src=\"Figures/journal.pone.0152173.g003.PNG\" alt=\"Types of anomaly\" style=\"width: 1000px\">\n",
    "  <figcaption>Fig 3. A taxonomy of unsupervised anomaly detection algorithms comprising of four main groups.\n",
    "Note that CMGOS can be categorized in two groups: It is a clustering-based algorithm as well as estimating a subspace of each cluster.\n",
    "    </figcaption>\n",
    "</figure> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d8030747d98c7bc24e5ffb4ddaa4d874",
     "grade": false,
     "grade_id": "cell-a8796ccd133e2263",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# used for manipulating directory paths\n",
    "import os\n",
    "\n",
    "# Scientific and vector computation for python\n",
    "import numpy as np\n",
    "\n",
    "# Plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Optimization module in scipy\n",
    "from scipy import optimize\n",
    "\n",
    "# will be used to load MATLAB mat datafile format\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# tells matplotlib to embed plots within the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5d68cdef25cb7271a62b66960048fe5a",
     "grade": false,
     "grade_id": "cell-67c1e260f2546f72",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Statistical Method: Gaussian model (Mahalanobis distance)\n",
    "\n",
    "In this exercise, you will implement an anomaly detection algorithm to detect anomalous behavior in the wine dataset. Among the 13 features we'll select 2 to keep the data two dimensional and explore how the algorithm works. You suspect that the vast majority of these examples are “normal” (non-anomalous) examples of wine types, but there might also be some examples of anomalous wines within this dataset, (whatever that means).\n",
    "\n",
    "You will use a Gaussian model to detect anomalous examples in your dataset. On the 2D dataset you will fit a Gaussian distribution and then find values that have very low probability and hence can be considered anomalies. After that, you will apply the anomaly detection algorithm to a larger dataset with many dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1f044db458587871ec27e4dedc791674",
     "grade": false,
     "grade_id": "cell-7df809fc7cca3b25",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#  The following command loads the dataset.\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# Select the first class (59 elements), columns 'Malic acid' and 'Proline' (see DESCR)\n",
    "X = load_wine()['data'][:59, [1,12]]\n",
    "\n",
    "#  Visualize the example dataset\n",
    "plt.plot(X[:,0], X[:,1], 'bx', mew=2, mec='k', ms=6)\n",
    "plt.axis([0, 5, 400, 1900])\n",
    "plt.xlabel('Malic acid')\n",
    "plt.ylabel('Proline');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6582168331e58d84a05367d961269aec",
     "grade": false,
     "grade_id": "cell-1ba6e30fed7007ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Multivariate Gaussian distribution\n",
    "\n",
    "To perform anomaly detection, you will first need to fit a model to the data's distribution. The Multivariate Gaussian distribution is given by\n",
    "$$\n",
    "p\\left( x; \\mu, \\Sigma \\right)  = \\frac{1}{  \\sqrt{  \\left( 2\\pi\\right)^k  \\left|\\Sigma\\right|  }}\n",
    "\\exp\\left(-\\frac{1}{2} \\left(x - \\mu \\right)^T \\Sigma^{-1}\\left(x - \\mu \\right)\\right)\n",
    "$$\n",
    "\n",
    "where $\\mu$ is the mean vector and $\\Sigma$ is the covariance matrix.\n",
    "\n",
    "### 1.2 Estimating parameters for a Gaussian \n",
    "\n",
    "You can estimate the parameters by using the following equations. To estimate the mean, you will use: \n",
    "\n",
    "$$ \\mu_i = \\frac{1}{m} \\sum_{j=1}^m x_i^{(j)},$$\n",
    "\n",
    "and for the covariance you will use:\n",
    "\n",
    "$$ \\Sigma = \\frac{1}{n}\\left(X - 1 \\mu^T \\right)^T\\left(X - 1 \\mu^T \\right)$$\n",
    "\n",
    "Your task is to complete the code in the function `estimateGaussian`. This function takes as input the data matrix `X` and should output an n-dimension vector `mu` that holds the mean for each of the $n$ features and the nxn covariance matrix `Sigma` that holds the variances of each of the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "70549ff2d2034ab8ff4aad03111bc25a",
     "grade": false,
     "grade_id": "cell-82cf0f769395c506",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def estimateGaussian(X):\n",
    "    \"\"\"\n",
    "    This function estimates the parameters of a Gaussian distribution\n",
    "    using a provided dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The dataset of shape (m x n) with each n-dimensional \n",
    "        data point in one row, and each total of m data points.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mu : array_like \n",
    "        A vector of shape (n,) containing the means of each dimension.\n",
    "    \n",
    "    Sigma : array_like\n",
    "        The (n x n) covariance matrix.\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Compute the mean of the data and the variances\n",
    "    In particular, mu[i] should contain the mean of\n",
    "    the data for the i-th feature and Sigma[i,j]\n",
    "    should contain covariance between the i-th and \n",
    "    the j-th feature.\n",
    "    \"\"\"\n",
    "    # Useful variables\n",
    "    m, n = X.shape\n",
    "    \n",
    "    # You should return these values correctly\n",
    "    mu = np.zeros(n)\n",
    "    Sigma = np.zeros((n, n))\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return mu, Sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "660a2d7346a5e2d75f28420a795d8912",
     "grade": false,
     "grade_id": "cell-7af1428259c9abac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Once you have completed the code in `estimateGaussian`, the next cell will visualize the contours of the fitted Gaussian distribution. \n",
    "\n",
    "From your plot, you can see that most of the examples are in the region with the highest probability, while\n",
    "the anomalous examples are in the regions with lower probabilities.\n",
    "\n",
    "To do the visualization of the Gaussian fit, we first estimate the parameters of our assumed Gaussian distribution, then compute the probabilities for each of the points and then visualize both the overall distribution and where each of the points falls in terms of that distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "25729d76a55e899a1e5acaa5b3547222",
     "grade": false,
     "grade_id": "cell-faac083b55d371de",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def multivariateGaussian(X, mu, Sigma):\n",
    "    \"\"\"\n",
    "    Computes the probability density function of the multivariate gaussian distribution.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The dataset of shape (m x n). Where there are m examples of n-dimensions.\n",
    "\n",
    "    mu : array_like\n",
    "        A vector of shape (n,) contains the means for each dimension (feature).\n",
    "\n",
    "    Sigma : array_like\n",
    "        Either a vector of shape (n,) containing the variances of independent features\n",
    "        (i.e. it is the diagonal of the correlation matrix), or the full\n",
    "        correlation matrix of shape (n x n) which can represent dependent features.\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    p : array_like\n",
    "        A vector of shape (m,) which contains the computed probabilities at each of the\n",
    "        provided examples.\n",
    "    \"\"\"\n",
    "    k = mu.size\n",
    "\n",
    "    # if sigma is given as a diagonal, compute the matrix\n",
    "    if Sigma.ndim == 1:\n",
    "        Sigma = np.diag(Sigma)\n",
    "\n",
    "    X = X - mu\n",
    "    p = (2 * np.pi) ** (- k / 2) * np.linalg.det(Sigma) ** (-0.5)\\\n",
    "        * np.exp(-0.5 * np.sum(X @ np.linalg.pinv(Sigma) * X, axis=1))\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b9b72e7ee5dbe95646cd23aa1c8e4618",
     "grade": true,
     "grade_id": "cell-2150bf578407629d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#  Estimate my and sigma2\n",
    "mu, Sigma = estimateGaussian(X)\n",
    "\n",
    "#  Returns the density of the multivariate normal at each data point (row) \n",
    "#  of X\n",
    "p = multivariateGaussian(X, mu, Sigma)\n",
    "\n",
    "#  Visualize the fit\n",
    "X1, X2 = np.meshgrid(np.linspace(0, 5, 100), np.linspace(400, 1900, 100))\n",
    "Z = multivariateGaussian(np.stack([X1.ravel(), X2.ravel()], axis=1), mu, Sigma)\n",
    "Z = Z.reshape(X1.shape)\n",
    "\n",
    "plt.plot(X[:, 0], X[:, 1], 'bx', mec='b', mew=2, ms=8)\n",
    "\n",
    "if np.all(abs(Z) != np.inf):\n",
    "    plt.contour(X1, X2, Z, levels=np.geomspace(1e-10, 1, 100), zorder=100)\n",
    "plt.xlabel('Malic acid');\n",
    "plt.ylabel('Proline');\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c4ffab09ad4b23750239330adaf29d98",
     "grade": false,
     "grade_id": "cell-bcd1978a25e5c0ca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "There is a problem with the previous estimation of our normal distribution. By inspecting the plot, it is clear that if outliers where removed the shape of the distribution should be an ellipse almost aligned with the y-axis. The problem is that the estimation of the covariance matrix is heavily influenced by the outliers. In principle, we would like to estimate the covariance using only the clean subset of the data.\n",
    "\n",
    "One way to solve this problems is to use a robust estimator for the covariance matrix called the Minimum Covariance Determinant. The Minimum Covariance Determinant estimator is a robust, high-breakdown point (i.e. it can be used to estimate the covariance matrix of highly contaminated datasets, up to $\\frac{n_\\text{samples} - n_\\text{features} - 1}{2}$ outliers) estimator of covariance. The idea is to find $\\frac{n_\\text{samples}+n_\\text{features}+1}{2}$ observations whose empirical covariance has the smallest determinant, yielding a \"pure\" subset of observations from which to compute standards estimates of location and covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a5670d93b896f1a41682f5d95da0514e",
     "grade": false,
     "grade_id": "cell-91c4d903ff4eec7b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.covariance import MinCovDet\n",
    "\n",
    "def estimateGaussianRobust(X):\n",
    "    \n",
    "    robust_cov = MinCovDet().fit(X)\n",
    "    mu = robust_cov.location_\n",
    "    Sigma = robust_cov.covariance_\n",
    "    \n",
    "    return mu, Sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2599de574a4029b8499c4fb84aabe588",
     "grade": false,
     "grade_id": "cell-65e6d5fb176fb9ce",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#  Estimate my and sigma2\n",
    "mu, Sigma = estimateGaussianRobust(X)\n",
    "\n",
    "#  Returns the density of the multivariate normal at each data point (row) \n",
    "#  of X\n",
    "p = multivariateGaussian(X, mu, Sigma)\n",
    "\n",
    "#  Visualize the fit\n",
    "X1, X2 = np.meshgrid(np.linspace(0, 5, 100), np.linspace(400, 1900, 100))\n",
    "Z = multivariateGaussian(np.stack([X1.ravel(), X2.ravel()], axis=1), mu, Sigma)\n",
    "Z = Z.reshape(X1.shape)\n",
    "\n",
    "plt.plot(X[:, 0], X[:, 1], 'bx', mec='b', mew=2, ms=8)\n",
    "\n",
    "if np.all(abs(Z) != np.inf):\n",
    "    plt.contour(X1, X2, Z, levels=np.geomspace(1e-10, 1, 10), zorder=100)\n",
    "plt.xlabel('Malic acid');\n",
    "plt.ylabel('Proline');\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5663e811275e16c700daad5f1be6f595",
     "grade": false,
     "grade_id": "cell-153e514d7c9e4ff3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Choosing the threshold\n",
    "\n",
    "Now, the outliers are identified as the points having the lowest probability according to the fitted model. This probabilities can serve as the scores of our method. Because of the \"squashing\" effect of the exponential, its much clearer to use the Mahalanobis distance, which is just the exponent of the multivariate gaussian.\n",
    "$$\n",
    "MD(x) = \\sqrt{\\left(x - \\mu \\right)^T\\Sigma^{-1}\\left(x - \\mu\\right)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "318091ed86c78b8d5b6b74db0cfadc5f",
     "grade": false,
     "grade_id": "cell-a0e4f41a71bcb28c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def MD(X):\n",
    "    mu, Sigma = estimateGaussianRobust(X)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return MD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d72c3641509812d02d97e07aa99f68d",
     "grade": false,
     "grade_id": "cell-94134ed90f77fe84",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the MD for each point, the plot the scores to identify possible outliers.\n",
    "\n",
    "scores = MD(X)\n",
    "threshold = 5\n",
    "\n",
    "plt.axhline(threshold, color='red')\n",
    "plt.plot(scores, 'o')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Mahalanobis distance')\n",
    "\n",
    "n_outliers = (scores > threshold).sum()\n",
    "top_outliers = np.argsort(scores)[::-1][:n_outliers]\n",
    "print('Top outliers: ', top_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fa2faa0ce631397dd6a4f0794e7cb7d8",
     "grade": false,
     "grade_id": "cell-062e9f575621b013",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8aca8cd46055bf98145829542a4b29f9",
     "grade": false,
     "grade_id": "cell-c3e3342ad286adb9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "First, for every record in the dataset, the k-nearest-neighbors have to be found. Then, an anomaly score is computed using these neighbors, whereas two possibilities have been proposed: Either the distance to the kth-nearest-neighbor is used (a single one) or the average distance to all of the k-nearest-neighbors is computed. In the following, we refer to the first method as kth-NN and the latter as k-NN. In practical applications, the k-NN method is often preferred. However, the absolute value of the score depends very much on the dataset itself, the number of dimensions, and on normalization. As a result, it is in practice not easy to select an appropriate threshold, if required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5eed682cfc52c7b8ae81524f77215166",
     "grade": false,
     "grade_id": "cell-8de17634a257a98c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Our KNN algorithm will use a brute force approach to calculate the nearest neighbors. This implies the calculation of $N^2$ pairwise distances among all observations (rows) of the data matrix. More efficient approaches for large datasets employ KDTrees or BallTrees (https://scikit-learn.org/stable/modules/neighbors.html). Euclidian distance is often used, but other metrics can be employed as well. The distance matrix is an $N\\times N$ matrix defined as:\n",
    "$$\n",
    "D = \\begin{pmatrix}\n",
    "d(x_1,x_1) & d(x_1,x_2) & \\dots & d(x_1,x_n)\\\\\n",
    "d(x_2,x_1) & d(x_2,x_2) & \\dots & d(x_2,x_n)\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "d(x_n,x_1) & d(x_n,x_2) & \\dots & d(x_n,x_n)\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "There exists an efficient way to calculate this matrix using Numpy broadcasting. To do this, we need to transform our original data matrix $X$ into new 3D arrays that repeat X along a given dimension. The purpose is to obtain 3D array with all elements of the form $x_{ik} - x_{jk}$ indexed as $ijk$. If you are unfamiliar with broadcasting operations, you may wish to implement this as a nested for loop, at the cost of being slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c04d4788c419f6c765e9327d057cbb1d",
     "grade": false,
     "grade_id": "cell-61c55d177b3c29bb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def distance_matrix(X):\n",
    "    \"\"\"Returns the distance matrix from a data matrix X. \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ec71253c52591e4c58e14ed1ec0ec4fa",
     "grade": false,
     "grade_id": "cell-1684d80c7ebbc05b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### K$^{th}$NN algorithm\n",
    "\n",
    "For the  K$^{th}$NN algorithm we need to find the distance to the kth neighbor of each observation, then sort according to those scores. The larger the distance, the more likely a point is to be an outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "751b6fc08d5ac02f8587426ce0539204",
     "grade": false,
     "grade_id": "cell-0b9bc33a64889101",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def scores_kthnn(X, k):\n",
    "    \"\"\" Return the oulier scores given by the distance to the kth neighbor.\"\"\"\n",
    "    \n",
    "    # Find the distance matrix\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Sort the distance matrix row by row.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Take the k column (excluding self distance)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "21c8481844b91b42aa62863ab5ffca32",
     "grade": false,
     "grade_id": "cell-6338e052c991c90f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### KNN algorithm\n",
    " For the KNN algorithm we need the average distance from the first k neighbors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "12fc3412cb95efaacd113f89e55929b7",
     "grade": false,
     "grade_id": "cell-df6cf5d946b9b6e1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def scores_knn(X, k):\n",
    "    \"\"\" Return the oulier scores given by the average distance to first k neighbors.\"\"\"\n",
    "    \n",
    "    # Find the distance matrix\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Sort the distance matrix row by row.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Take the average up to the k column (excluding self distance)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9ec57f9c7e216f68791eec368ebd0083",
     "grade": false,
     "grade_id": "cell-0cd06c323411178d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The choice of the parameter k is of course important for the results. If it is chosen too low, the density estimation for the records might be not reliable. On the other hand, if it is too large, density estimation may be too coarse. As a rule of thumb, k should be in the range 10 < k < 50.\n",
    "\n",
    "Testing KNN in the wine dataset should output the following result:\n",
    "\n",
    "![](Knn-outliers.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a9175f9af6aad6264a3cd45f7e1c0106",
     "grade": false,
     "grade_id": "cell-cb7242301a7286bd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Normalize the data matrix, this step is important\n",
    "# since the scale of the two columns differs significantly\n",
    "from sklearn import preprocessing\n",
    "\n",
    "X_scaled = preprocessing.MinMaxScaler().fit_transform(X)\n",
    "\n",
    "k = 10\n",
    "kthnn_scores = scores_kthnn(X_scaled, k)\n",
    "knn_scores = scores_knn(X_scaled, k)\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], s=1000*kthnn_scores, edgecolors='r',\n",
    "            facecolors='none', label='Outlier scores kthnn')\n",
    "plt.scatter(X[:,0], X[:,1], s=1000*knn_scores, edgecolors='b',\n",
    "            facecolors='none', label='Outlier scores knn')\n",
    "plt.scatter(X[:,0], X[:,1], s=5, color='k', label='Data points')\n",
    "legend = plt.legend()\n",
    "legend.legendHandles[0]._sizes = [20]\n",
    "legend.legendHandles[1]._sizes = [20]\n",
    "\n",
    "plt.axis([0, 5, 400, 1900])\n",
    "plt.xlabel('Malic acid')\n",
    "plt.ylabel('Proline');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a1598d5a6e80678111d732d4cddca73a",
     "grade": false,
     "grade_id": "cell-fb7fbd40533d50e3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## LOF\n",
    "\n",
    "The local outlier factor is the most well-known local anomaly detection algorithm and also introduced the idea of local anomalies first.\n",
    "To calculate the LOF score, three steps have to be computed:\n",
    "1. The k-nearest-neighbors have to be found for each record x. In case of distance tie of the kth neighbor, more than k neighbors are used.\n",
    "2. Using these k-nearest-neighbors $N_k$, the local density for a record is estimated by computing the local reachability density (LRD):\n",
    "$$\n",
    "LRD_k(x) = 1/\\left(  \\frac{\\sum\\limits_{o\\in N_k(x)} d_k(x,o)}{\\left|N_k(x)\\right|} \\right)\n",
    "$$\n",
    "whereas dk(·) is the reachability distance. Except for some very rare situations in highly dense clusters, this is the Euclidean distance.\n",
    "3. Finally, the LOF score is computed by comparing the LRD of a record with the LRDs of its k neighbors:\n",
    "$$\n",
    "LOF(x) = \\frac{\\sum\\limits_{o\\in N_k(x)}\\frac{LRD_k(o)}{LRD_k(x)}}{\\left|N_k(x)\\right|}\n",
    "$$\n",
    "\n",
    "The LOF score is thus basically a ratio of local densities. This results in the nice property of LOF, that normal instances, which densities are as big as the densities of their neighbors, get a score of about 1.0. Anomalies, which have a low local density, will result in larger scores. At this point it is also clear why this algorithm is local: It only relies on its direct neighborhood and the score is a ratio mainly based on the k neighbors only. Of course, global anomalies can also be detected since they also have a low LRD when comparing with their neighbors. It is important to note that in anomaly detection tasks, where local anomalies are not of interest, this algorithm will generate a lot of false alarms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a5c6bfad4b457fd32a94b65cadeb33ac",
     "grade": false,
     "grade_id": "cell-9c2e926d6ff59112",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The reachability distance is defined as:\n",
    "$$\n",
    "d_k(x, o) = \\max\\left( k-distance(o), d(x,o)  \\right)\n",
    "$$\n",
    "\n",
    "In words, the reachability distance of an object $x$ from $o$ is the true distance of the two objects, but at least the k-distance of $o$. Objects that belong to the k nearest neighbors of $o$ (the \"core\" of $o$) are considered to be equally distant, i.e., equally reachable from $o$. The reason for this distance is to get more stable results. Note that this is not a distance in the mathematical definition, since it is not symmetric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e17e16252d460a5901fd6c4997e76b2e",
     "grade": false,
     "grade_id": "cell-e8632fd1c3984a8a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def lof(X, k):\n",
    "    # Find the distance matrix\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Sort the distance matrix row by row to obtain the k neighborhood of each row.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "        \n",
    "    # Store the k-distance of each observation\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Also store the indices of the neighbors to find the k-distance of each point o.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "       \n",
    "    # Find the reachability distances of each neighborhood\n",
    "    # Note: Numpy fancy indexing is your friend\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    #print(r_dist)\n",
    "    \n",
    "    # Find LDR for each observation\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    #Find LOF scores\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b4cef27660ec142ca63db57f9103a193",
     "grade": false,
     "grade_id": "cell-7a8a857b280d9c2a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Normalize the data matrix, this step is important\n",
    "# since the scale of the two columns differs significantly\n",
    "from sklearn import preprocessing\n",
    "\n",
    "X_scaled = preprocessing.MinMaxScaler().fit_transform(X)\n",
    "\n",
    "k = 10\n",
    "lof_scores = scores_kthnn(X_scaled, k)\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], s=1000*lof_scores, edgecolors='r',\n",
    "            facecolors='none', label='Outlier scores')\n",
    "plt.scatter(X[:,0], X[:,1], s=5, color='k', label='Data points')\n",
    "legend = plt.legend()\n",
    "legend.legendHandles[0]._sizes = [20]\n",
    "\n",
    "plt.axis([0, 5, 400, 1900])\n",
    "plt.xlabel('Malic acid')\n",
    "plt.ylabel('Proline');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9eb8ddee342720e9ea1b5b0a995b116d",
     "grade": false,
     "grade_id": "cell-a7dd90b20b5e3129",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## ABOD\n",
    "\n",
    "The main idea behind ABOD is that if $x$ is an outlier, the variance of angles between pairs of the remaining objects becomes small:\n",
    "\n",
    "![](Figures/abod.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "66de358dbb0db4537739caf2780b0776",
     "grade": false,
     "grade_id": "cell-c70910f90145d1ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For a point within a cluster, the angles between\n",
    "difference vectors to pairs of other points differ widely. The\n",
    "variance of the angles will become smaller for points at the\n",
    "border of a cluster. However, even here the variance is still\n",
    "relatively high compared to the variance of angles for real\n",
    "outliers. Here, the angles to most pairs of points will be\n",
    "small since most points are clustered in some directions.\n",
    "\n",
    "As a result of these considerations, an angle-based outlier factor (ABOF) can describe the\n",
    "divergence in directions of objects relatively to one another.\n",
    "If the spectrum of observed angles for a point is broad, the\n",
    "point will be surrounded by other points in all possible directions meaning the point is positioned inside a cluster. If\n",
    "the spectrum of observed angles for a point is rather small,\n",
    "other points will be positioned only in certain directions.\n",
    "This means, the point is positioned outside of some sets of\n",
    "points that are grouped together. Thus, rather small angles\n",
    "for a point that are rather similar to one another imply\n",
    "that such point is an outlier.\n",
    "\n",
    "ABOD has been proposed as able to perform outlier detection more reliably in high dimensional data sets than distance based methods.\n",
    "\n",
    "A problem of the basic approach ABOD is obvious: since\n",
    "for each point all pairs of points must be considered, the\n",
    "time-complexity is in O($n^3$), the original ABOD paper proposes two approximations to address this problem: FastABOD and LB-ABOD. These will not be discussed here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4d7132a953485392cad538f0500fdc6e",
     "grade": false,
     "grade_id": "cell-a9dd6e10ad54886c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### The Angled Based Outlier Factor (ABOF)\n",
    "\n",
    "As an approach to assign the ABOF value to any object\n",
    "in the database $\\mathcal{D}$, we compute the scalar product of the\n",
    "difference vectors of any triple of points (i.e. a query point $\\vec{A} \\in \\mathcal{D}$\n",
    "and all pairs $(\\vec{B},\\vec{C})$ of all remaining points in $\\mathcal{D} \\backslash \\{\\vec{A}\\})$\n",
    "normalized by the quadratic product of the length of\n",
    "the difference vectors, i.e. the angle is weighted less if the\n",
    "corresponding points are far from the query point. By this\n",
    "weighting factor, the distance influences the value after all,\n",
    "but only to a minor part. Nevertheless, this weighting of\n",
    "the variance is important since the angle to a pair of points\n",
    "varies naturally stronger for a bigger distance. The variance\n",
    "of this value over all pairs for the query point $\\vec{A}$\n",
    "constitutes the angle-based outlier factor (ABOF) of $\\vec{A}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "afd22e4228cbfeca82652e528a2f27bd",
     "grade": false,
     "grade_id": "cell-fe36555de0157590",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "$$\n",
    "ABOF(\\vec{A}) = VAR_{\\vec{B},\\vec{C}\\in\\mathcal{D}} \\left( \\frac{\\left<\\overline{AB}, \\overline{AC}\\right>}\n",
    "{\\left\\Vert \\overline{AB}  \\right\\Vert^2 \\cdot \\left\\Vert \\overline{AC}  \\right\\Vert^2} \\right)\\\\\n",
    "= \\frac\n",
    "{\\sum_B\\sum_C\\left(\n",
    "\\frac{1}{\\left\\Vert \\overline{AB}  \\right\\Vert \\cdot \\left\\Vert \\overline{AC}  \\right\\Vert}\n",
    "\\frac{\\left<\\overline{AB}, \\overline{AC}\\right>}{\\left\\Vert \\overline{AB}  \\right\\Vert^2 \\cdot \\left\\Vert \\overline{AC}  \\right\\Vert^2} \n",
    "\\right)^2}\n",
    "{\\sum_B\\sum_C\\frac{1}{\\left\\Vert \\overline{AB}  \\right\\Vert \\cdot \\left\\Vert \\overline{AC}  \\right\\Vert}} - \n",
    "\\left(\\frac\n",
    "{\\sum_B\\sum_C\n",
    "\\frac{1}{\\left\\Vert \\overline{AB}  \\right\\Vert \\cdot \\left\\Vert \\overline{AC}  \\right\\Vert}\n",
    "\\frac{\\left<\\overline{AB}, \\overline{AC}\\right>}{\\left\\Vert \\overline{AB}  \\right\\Vert^2 \\cdot \\left\\Vert \\overline{AC}  \\right\\Vert^2} \n",
    "}\n",
    "{\\sum_B\\sum_C\\frac{1}{\\left\\Vert \\overline{AB}  \\right\\Vert \\cdot \\left\\Vert \\overline{AC}  \\right\\Vert}}\\right)^2\n",
    "$$\n",
    "\n",
    "__NOTE__: This way of weighting the cosine similar is weird in my opinion. In fact, the pyod package implements ABOD without these weights. I'm not sure which way is the correct one, or even is one can say that either can be wrong, since the constructions of the algorithm is not based in any formalism. I have yet yo find a discussion about the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2be7e68dfd2636b34ee54522edca48c7",
     "grade": false,
     "grade_id": "cell-60f367705ae41844",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def abof(a, X):\n",
    "    \"\"\" Returns abof score for X[a] \"\"\"\n",
    "    \n",
    "    m = len(X)\n",
    "    # Variablea to store the first and second terms of the varince\n",
    "    var1 = 0\n",
    "    var2 = 0\n",
    "    # variable to store the normalization constant (sum of weights)\n",
    "    norm_c = 0\n",
    "    \n",
    "    # Loop over all pairs of points\n",
    "    for b in range(m):\n",
    "        if a == b:\n",
    "            continue\n",
    "        for c in range(b+1, m):\n",
    "            if a == c:\n",
    "                continue\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "            \n",
    "    var = var1/norm_c - (var2/norm_c)**2\n",
    "    \n",
    "    return var            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d5403815401a92c8f35025e16f27e0bf",
     "grade": false,
     "grade_id": "cell-5219a808b1a3d06a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def abod(X):\n",
    "    \"\"\" Retrun abof scores for X \"\"\"\n",
    "    \n",
    "    scores = []\n",
    "    for a in range(len(X)):\n",
    "        scores.append(abof(a, X))\n",
    "\n",
    "    return np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9ff2cca578bc7aa44bc8ff0e9c97d628",
     "grade": false,
     "grade_id": "cell-68cebbfd001ef8aa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Normalize the data matrix, this step is important\n",
    "# since the scale of the two columns differs significantly\n",
    "from sklearn import preprocessing\n",
    "\n",
    "X_scaled = preprocessing.MinMaxScaler().fit_transform(X)\n",
    "\n",
    "k = 10\n",
    "abod_scores = abod(X_scaled)\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c=np.log(abod_scores))\n",
    "plt.colorbar()\n",
    "\n",
    "plt.axis([0, 5, 400, 1900])\n",
    "plt.xlabel('Malic acid')\n",
    "plt.ylabel('Proline');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a354cffa51ae9f1dd5577dcdbc2a602e",
     "grade": false,
     "grade_id": "cell-084c79c858c046ff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## iForest\n",
    "\n",
    "One efficient way of performing outlier detection in high-dimensional datasets is to use random forests. The Isolation Forest algorithm 'isolates' observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature.\n",
    "\n",
    "Since recursive partitioning can be represented by a tree structure, the number of splittings required to isolate a sample is equivalent to the path length from the root node to the terminating node.\n",
    "\n",
    "This path length, averaged over a forest of such random trees, is a measure of normality and our decision function.\n",
    "\n",
    "Random partitioning produces noticeably shorter paths for anomalies. Hence, when a forest of random trees collectively produce shorter path lengths for particular samples, they are highly likely to be anomalies.\n",
    "\n",
    "An example of random partitioning in a 2D dataset of normally distributed points is given below for a non-anomalous point:\n",
    "\n",
    "![](Figures/Isolating_a_Non-Anomalous_Point.png)\n",
    "\n",
    "Another example for a point that's more likely to be an anomaly is now shown:\n",
    "\n",
    "![](Figures/Isolating_an_Anomalous_Point.png)\n",
    "\n",
    "It is apparent from the pictures how anomalies require fewer random partitions to be isolated, compared to normal points. \n",
    "\n",
    "From a mathematical point of view, recursive partitioning can be represented by a tree structure named Isolation Tree, while the number of partitions required to isolate a point can be interpreted as the length of the path, within the tree, to reach a terminating node starting from the root. \n",
    "\n",
    "More formally, let $X = \\{ x_1, \\ldots,  x_n \\}$ be a set of d-dimensional points and $X' \\subset X$ a subset of $X$. An Isolation Tree (iTree) is defined as a data structure with the following properties: \n",
    "\n",
    "1. for each node $T$ in the Tree, $T$ is either an external-node with no child, or an internal-node with one \"test\" and exactly two daughter nodes ($T_l$, $T_r$)\n",
    "2. a test at node $T$ consists of an attribute $q$ and a split value $p$ such that the test $q < p$ determines the traversal of a data point to either $T_l$ or $T_r$.\n",
    "\n",
    "In order to build an iTree, the algorithm recursively divides $X'$ by randomly selecting an attribute $q$ and a split value $p$, until either (i) the node has only one instance or (ii) all data at the node have the same values. \n",
    "\n",
    "When the iTree is fully grown, each point in $X$ is isolated at one of the external nodes. Intuitively, the anomalous points are those (easier to isolate, hence) with the smaller path length in the tree, where the path length $h(x_i)$ of point $x_{i}\\in X$ is defined as the number of edges $x_i$ traverses from the root node to get to an external node. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b2ebd92a2930e7368721ad9f55ad05da",
     "grade": false,
     "grade_id": "cell-0437b0b29c7a11f6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## References\n",
    "\n",
    "- A Comparative Evaluation of Unsupervised Anomaly Detection Algorithms for Multivariate Data (https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0152173)\n",
    "- https://scikit-learn.org/stable/modules/outlier_detection.html\n",
    "- https://pyod.readthedocs.io/en/latest/\n",
    "- https://github.com/yzhao062/anomaly-detection-resources\n",
    "- https://arxiv.org/abs/1709.07045\n",
    "- https://scikit-learn.org/stable/modules/neighbors.html\n",
    "- https://en.wikipedia.org/wiki/Local_outlier_factor\n",
    "- Angle-Based Outlier Detection in High-dimensional Data, Kriegel et.al.\n",
    "- https://en.wikipedia.org/wiki/Isolation_forest\n",
    "- Isolation Forest, Liu and Zhou"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
